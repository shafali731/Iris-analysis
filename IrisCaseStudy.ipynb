{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Iris Dataset Case Study\n\n## Hello World of Machine Learning: Classification of Iris Flowers\nWelcome to Machine Learning! \n\nThe Iris Dataset is one of the most famous datasets in the machine learning community. This dataset consists of the physical parameters of three species of flower — Versicolor, Setosa and Virginica. The numeric parameters which the dataset contains are Sepal width, Sepal length, Petal width and Petal length. \n\nIn this excercise, we'll predict the species of the flowers based on these parameters. The data consists of continuous numeric values which describe the dimensions of the respective features. We will be training the model based on these features.\n\n### What makes this excercise doable:\n* Attributes are numeric\n* This classification problem introduces fundamental supervised learning algorithms\n* It only has 4 attributes and 150 rows - small, easy to explore, and not memory intensive\n* All numeric attributes are of the same unit scale, not requiring any special scaling or transformations\n\n### Summary:\nIn this step-by-step tutorial you will:\n* Download and install Python SciPy and get the most useful package for machine learning in Python.\n* Load a dataset and understand it’s structure using statistical summaries and data visualization.\n* Create 6 machine learning models, pick the best and build confidence that the accuracy is reliable.\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 1. Create a virtual env and install libraries\n* scipy\n* numpy\n* matplotlib\n* pandas\n* sklearn\n* seaborn"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 2. Load the data and import the libraries\nPro tip: the sklearn library \n`import seaborn as sns\niris = sns.load_dataset('iris')`"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 3. Summarize the Dataset\n1. Dimensions of the dataset.\n2. Peek at the data itself.\n3. Statistical summary of all attributes.\n4. Breakdown of the data by the class variable."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 4. Data Visualization\n1. Univariate plots to better understand each attribute.\n2. Multivariate plots to better understand the relationships between attributes."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 5. Evaluate Some Algorithms\n1. Separate out a validation dataset.\n\tWe will split the loaded dataset into two, 80% of which we will use to train, \tevaluate and select among our models, and 20% that we will hold back as a \tvalidation dataset.\n1. Set-up the test harness to use 10-fold cross validation.\n2. Build multiple different models to predict species from flower measurements\n3. Select the best model."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}